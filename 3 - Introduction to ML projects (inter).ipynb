{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ML projects\n",
    "\n",
    "<img src=\"https://www.mrtfuelcell.polimi.it/images/logo_poli.jpg\" height=\"200\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg\" height=\"150\">\n",
    "\n",
    "A2A ML Course - day 3 - 04/10/2024\n",
    "\n",
    "Author: Maciej Sakwa <br>\n",
    "Coauthors: Micheal Wood, Emanuele Ogliari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Machine Learining project structure\n",
    "2. Hands-on ML project:\n",
    "    - Getting the data and problem definition\n",
    "    - Explorative Data Analysis\n",
    "    - Feature Engineering\n",
    "    - Model development\n",
    "    - Results\n",
    "\n",
    "## Learning obejctives\n",
    "\n",
    "* Understand the necessary steps to complete a ML project\n",
    "* Estimate the timeframe required for a ML project\n",
    "* First hand experience with programming a simple ML project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://freesvg.org/img/Brain-Computer.png\" width=\"300\">\n",
    "\n",
    "Despite the numerous possible projects that can be solved using Machine Learning and Deep Learning the general project structure to follow will be almost the same every time. \n",
    "\n",
    "Surprisingly, the the biggest decisions that have to be made are not about the ML or DL models, **they are about data**.\n",
    "\n",
    "This lesson is created to give you understanding of the necessary steps needed to acomplish any ML project no matter of the size of the dataset. With hands-on experience, you will be able to understand where lies the actual difficulty of ML projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/project%20structure.png?raw=true\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise the main steps can be described as:\n",
    "\n",
    "| N° | Step | Details |\n",
    "| --- | --- | --- |\n",
    "|1. | **GET DATA** | Acquire and curate the dataset. Define the problem to solve |\n",
    "|2. | **EDA** | Explore the data to understand how (and **if**!) you can use it to solve the problem |\n",
    "|3. | **FEATURE ENG.** | Transform the data to better fit your needs |\n",
    "|4. | **MODELING** |Develop ML/DL models, test them, fine tune them for better results |\n",
    "|5. | **PRESENT AND LAUNCH** | Get feedback from experts in topic, launch your project on the platform of choice|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **IF** in the second point is crucial for development of *useful* ML projects.\n",
    "ML is often called a **black-box**. However it is not a *magic* black-box.\n",
    "You can not solve *any* problem using *any* data. The data has to facilitate the solution of the problem.\n",
    "\n",
    ">**A reasonable data-driven problem definition is necessary for development of good ML projects.**\n",
    "\n",
    "Development of ML projects should come from cooperation between Data Scientists and Engineers who understand the topic through and through.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on ML project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the data and define the problem\n",
    "\n",
    "Any project has to start somewhere, an ML project starts with data acquisition. In reality, the process of data acquisition and preprocessing is a highly complex problem in itself. So big companies have people to do that for them (so called *Data Engineers*) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us, let's consider the step already done and let's begin with loading our csv datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace links with github*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load two datasets:\n",
    "1. The electricity generation in Spain\n",
    "2. The weather data from 5 big cities in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy = pd.read_csv('./data/energy_supply.csv')\n",
    "data_weather = pd.read_csv('./data/weather_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will serve you as a guideline to **supervised learning** branch of Machine Learning algorithms (sometimes also called **shalow learning**).\n",
    "\n",
    "**The task that we will try to perform is to predict the actual electricity price in Spain knowing the actual generation data (and weather parameters) in the country.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explorative data analysis\n",
    "\n",
    "In reality, it is very hard to *decouple* EDA from Feature Engineering. **It's a loop, an iterative process.**  However, for the sake of learning, as EDA let's just give a look to what we loaded. For now let's focus on the energy dataset data_energy. The other one (data_weather) will be usefull later.\n",
    "\n",
    "Try using the `.describe()` and `.info()` methods. Also list all the columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your code before the \"#\" sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a short excercise to refresh the pandas syntax. Do you remember that you can use a list to extract several columns? Let's try it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_list = [\n",
    "    'generation fossil brown coal/lignite',\n",
    "    'generation fossil coal-derived gas',\n",
    "    'generation fossil gas',\n",
    "    'generation fossil hard coal',\n",
    "    'generation fossil oil',\n",
    "    'generation fossil oil shale',\n",
    "    'generation fossil peat']\n",
    "\n",
    "hydro_list = [\n",
    "    'generation hydro pumped storage aggregated',\n",
    "    'generation hydro pumped storage consumption',\n",
    "    'generation hydro run-of-river and poundage',\n",
    "    'generation hydro water reservoir',\n",
    "    'generation marine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy_fossils = ... # Filter the data to contain only the fossil fuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, do you remember that you create new columns? For that reason we can use aggregative functions that we learned last time. Create the `'total'` column by using a `.sum()` function. <br> For it to work we have to specify the axis along which we sum. The column axis is the second one, so we have to wrice `axis=1` in the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy_fossils['total'] = ... # Create the new colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the dataset that we are working with is a **time series** dataset\n",
    "\n",
    "---\n",
    "\n",
    "**Working with time series data in pandas** \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:650/0*MJtKLn0wgompp9lJ.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series datasets are probably one of the most important and most common form of data that we can acquire. In fact, time flies (and does not stop) so it is quite obvious that people wanted to know and see how things change with time. Time series data do exacly that. Initially working with a new form of data might be a bit frightening. However, there is nothing to worry about! In fact, time series data is an organised form of tabular data. As pandas comes packed with tools to work with tabular data, it is not a surprise it can handle well time series datasets. <br> pandas has many built-in functions and tools to help us accomplish that.\n",
    "\n",
    "The rule of thumb when working with time series datasets in pandas is to make it the **index** of the Dataframe (so the first column).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.index = pd.to_datetime(data_energy['time'], utc=True)\n",
    "data_energy.drop(columns='time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this piece of code we set the **index** to be the time column. We switch it to date time format by using the function `pd.to_datetime()`. As the `'time'` column becomes redundant, we can remove it using the `.drop()` method.\n",
    "\n",
    "Having the datetime as the Dataframe index allows for easy time-based filtering, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.loc['2017-01-01':'2017-01-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an excercise, try to extract the first three months of 2016, and save it as `data_energy_time_slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy_time_slice = ... # Remove the dots and complete the query, remember to add .copy() at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful when we will try to plot the data.\n",
    "\n",
    "For example the code below, let's you plot the energy generation by source. We can use the time slicing to take a closer look to some periods of time. <br>Try to experiment with the plot. You can add or remove different columns, change the colors, scales, time range, ect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Single column data\n",
    "plt.plot(data_energy_time_slice['generation solar'], label='Solar')\n",
    "plt.plot(data_energy_time_slice['generation wind onshore'], label='Wind')\n",
    "plt.plot(data_energy_time_slice['generation biomass'], label='Biomass')\n",
    "plt.plot(data_energy_time_slice['generation nuclear'], label='Nuclear')\n",
    "\n",
    "# Aggregated data from our defined lists\n",
    "plt.plot(data_energy_time_slice[fossil_list].sum(axis=1), label='Fossils') \n",
    "plt.plot(data_energy_time_slice[hydro_list].sum(axis=1), label='Hydro')\n",
    "\n",
    "# Visuals\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power generation (MW)')\n",
    "plt.grid(which='major', alpha = 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the energy prices change in the same period. Try to plot the spot price and the day ahead price. The corresponding column names are `'price actual'` and `'price day ahead'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.plot(...)\n",
    "plt.plot(...)\n",
    "\n",
    "# Visuals\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cost (€/MW)')\n",
    "plt.grid(which='major', alpha = 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What are some conclusions we can draw here? Do you see some correlations?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the correlations by calling the .corr() function. By default, it calculates the pearsons coefficient $\\rho_{X, Y}$ between each two columns $X$ and $Y$, for all the columns in the dataframe:\n",
    "\n",
    "$$\n",
    " \\rho_{X, Y} = \\frac{cov(X, Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "Where $cov(X, Y)$ is the covariance of the two columns, and $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of columns. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of empty NaN values! It's either because a lot of values are missing, or the columns are 0 valued, let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of columns with 0-values! Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_columns = [\n",
    "    'generation fossil coal-derived gas', \n",
    "    'generation fossil oil shale', \n",
    "    'generation fossil peat', \n",
    "    'generation geothermal', \n",
    "    'generation hydro pumped storage aggregated', \n",
    "    'generation marine', \n",
    "    'generation wind offshore', \n",
    "    'forecast wind offshore eday ahead']\n",
    "\n",
    "data_energy.drop(columns=zero_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we got rid of the zero columns, let's calculate the `.corr()` function again. The output is also a Dataframe, so we can filter it the same way. Let's see what columns impact the actual price the strongest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.corr()['price actual'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the actual price is highly correlated positively with the day ahead price (obviously), but also with fossil fuel based generation, and total generation needed. So the more energy we need, and the more fossil fuels produce, the more expensive the energy gets.\n",
    "\n",
    "Also it's negavitely correlated with hydro-based generation, and with wind-based generation, they are cheap sources. Surprisingly the PV-based production has low impact on the price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's check for the missing data. Use the .info() and .dropna() methods to get rid of null-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.dropna(inplace=True)\n",
    "data_energy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the task a bit more contemporary let's assume we have no day-ahead knowledge of the system. Simply, let's remove the `'forecast'` columns and the day-ahead price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_list = [item for item in data_energy.columns.to_list() if 'forecast' in item] + ['price day ahead']\n",
    "forecast_list\n",
    "\n",
    "data_energy.drop(columns=forecast_list, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain some thing first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The problem that we are trying to solve can be classified as a standard **regression** task. In theoretical terms, all regression problems are a subset of a more broader Machine Learning group called **supervised learning**.\n",
    ">\n",
    ">In **supervised learning**, by the term *supervised* we mean that we have some prior knowledge on the desired outputs that we can use in modeling. It means, that we can teach the model to use some variables we have, to predict the desired output. In more mathematical terms, in **supervised learning** we have a set of variables $\\mathbf{X}$, a set of outputs $y$, and we are searching for a function $f(\\mathbf{X})$ that approximates the outputs $\\hat{y}$ using the inputs $\\mathbf{X}$. Of course, the approximation is almost never ideal, as most of the problems that we deal with are highly **nonlinear**, but still we are trying to minimize the difference between the real outputs and the approximated outputs $min(y - \\hat{y})$. In reality, we rarely search for the approximating functions on our own, there are countless **ML and DL models** ready to be used. *We only have to optimize the model to work on our data.*\n",
    ">\n",
    ">We often call the variables **features** or **inputs**, and the outputs **labels** or **targets**. Moreover, the process of optimizing the approximating function $f(\\mathbf{X})$ is often reffered to as **fitting** or **training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that theory laid out, let's define the **features** and **labels** for our prediction. \n",
    "\n",
    "In our case, we want to estimate the actual energy price using the generation data, let's extract the corresponding columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = data_energy.columns.to_list() \n",
    "input_columns.remove('price actual')    # Inputs are all columns except for the 'price actual'\n",
    "label_columns = ['price actual']        # Labels is the 'price actual' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data_energy[input_columns].copy()\n",
    "labels = data_energy[label_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we don't do any modifications to the **features** we have in the dataset, let's see what results we get with almost *raw* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Basics of Scikit Learn**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/refs/heads/main/images/Scikit_learn_logo_small.svg\" height=\"200\">\n",
    "\n",
    "\n",
    "Before we start processing data with the models, we should briefly discuss an amazing python package called **Scikit Learn**. Is is a very robust and surprisingly easy to use library that contains a plethora of basic ML algorithms and data transformations.  As with most of the common libraries, it has an excessive documentation website that can be found [here](https://scikit-learn.org/stable/). It has been first introduced in 2007, and obviously the ML and DL landscape changed significantly since then. \n",
    "\n",
    "However, **Scikit Learn** (or sklearn) to this day is a library often used by students and professionals alike due to its simplicity and efficiency, making it **a great entry point into the world of ML**.\n",
    "\n",
    "The characteristic trait of sklearn is a very rigid module.Class.function structure, where each model or data transformer is loaded as a python Class and various transformations are done by calling the functions. Some of the most common functions inlcude:\n",
    "\n",
    "- `.fit(X, y)` - fit a model to a dataset\n",
    "- `.transform(X)` - transform a dataset using a model (can be connected with `.fit()` via `.fit_transform()`)\n",
    "- `.predict(X)` - predicts the values from a given input\n",
    "- `.metric(y_1, y_2)` - calculates an error metric between y_1 and y_2, where the `.metric()` is a selected error type, e.g. `.mean_absolute_error(y_1, y_2)`\n",
    "\n",
    "Of course, this list is not exhaustive. With that, let's import some submodules from sklearn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, neighbors, ensemble, tree, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we input the data into the model there is one important transformation that we should't forget: **scaling**. \n",
    "\n",
    "Each column of our dataframe represents one **feature** of our inputs, and quite naturally each feature takes values from a certain range (or mathematically, they are sampled from a certain population). \n",
    "\n",
    "For example, the range of values in the column of `'generation nuclear'` is different from `'generation fossil hard coal'`, because the installed power of nuclear is different from hard coal. We understand that, but the model might have a difficult time understanding why some features are bigger then others, it will naturaly think the bigger features are more important. In this way, we are introducing **bias** into model fitting.\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/hist_fossil_fuels.png?raw=true\" height=\"200\">\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/hist_nuclear.png?raw=true\" height=\"200\">\n",
    "\n",
    "\n",
    "If we want the model to avoid this bias in fitting, we have to **scale** the input data. The two most common methods are:\n",
    " \n",
    "- **min-max scaling**\n",
    "- **standard scaling**\n",
    "\n",
    "\n",
    "If you are interested you can find more mathematical explainations in the appendix. In our case, we will use the StandardScaler() class from sklearn to scale the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler() # Set up the class instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `.fit_transform()` to transform our inputs. We do not have to transform the labels.\n",
    "\n",
    "> **NB:** The output of the scaler comes in the form of a numpy array object. The numpy package will be explained further down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_scaled = scaler.fit_transform(inputs) # Get the inputs\n",
    "labels_scaled = labels.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to divide the inputs and the labels in train and test subgroups. As the name implies, we use the first subgroup to train the model and the latter to test it. It should remain unseen in training.\n",
    "\n",
    "Typically the division is set to be **70-30**, or **80-20**. In time series data, we usualy cut off the newest part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(inputs) \n",
    "cutoff = int(0.7*n) # Set the cutoff threshold at 70% of the length of the input array\n",
    "\n",
    "train_inputs = inputs_scaled[:cutoff]   # Cut before\n",
    "train_labels = labels_scaled[:cutoff]   # Cut before\n",
    "\n",
    "test_inputs = inputs_scaled[cutoff:]    # Cut after\n",
    "test_labels = labels_scaled[cutoff:]    # Cut after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualise the division with the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, cutoff), train_inputs[:, 0])\n",
    "plt.plot(np.arange(cutoff, n), test_inputs[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is set up, the next step is usually benchmarking some of the common models that are used to solve similar problems.\n",
    "\n",
    "In theory, some theory knowledge is necessary to properly set up and use the models. In practice, everything can be easily done without knowing the theory behind the models. We can easily treat them like **black-boxes**.\n",
    "\n",
    "The general goal of this lesson is to get you familiar with the *structure* of the project, not with the ins and outs of the models. So for now we will just proceed with some selected models and see the results. However, if you want to learn more about the models that we use here, feel free to check out the appendix where you can find details on how each model functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can declare each model in the same way we set up the scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the parentheses, you can tweak some parameters (also called **hyperparameters**), but for now let's keep the default options.\n",
    "\n",
    "To train the model, we can call the `.fit()` method. As this is a *supervised* model, we specify the train inputs and labels. <br> *It might take a minute, but it depends on the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X=train_inputs, y=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call the `.predict()` method with test inputs to see how the model performs with unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr = lr.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly visualise the results with a plot (we can declare it in a function to quickly reuse it later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results:np.array, model_name:str, x_range=(0, 520)) -> None:   \n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plots\n",
    "    plt.plot(test_labels, label='True values')\n",
    "    plt.plot(results, label=model_name)\n",
    "\n",
    "    # Visuals\n",
    "    plt.xlim(x_range)\n",
    "    plt.xlabel('Test sample')\n",
    "    plt.ylabel('Price (€/MW)')\n",
    "    plt.grid(which='major', alpha = 0.5)\n",
    "\n",
    "    # Tidy up\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(results_lr, 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We examined the data, explored some features, then picked and trained a model, and displayed the results. **Buon lavoro!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are not done yet. Let's benchmark some other common simple models, and get some detailed metrics. \n",
    "\n",
    "Let's quickly save the things that we did as functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_pred(model, X_train, y_train, X_test):\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict with the model\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train several predictors to check how they perform on our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso()\n",
    "\n",
    "results_lasso = train_and_pred(lasso, train_inputs, train_labels, test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = tree.DecisionTreeRegressor()\n",
    "results_dtr = train_and_pred(dtr, train_inputs, train_labels, test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise:** Try to implement the K-Nearest Neighbors regressor, save the results as `results_knn`. You can find the syntax reference [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the models' results as a dictionary: <br>\n",
    "*You can add another row if you did the excercise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'LR': results_lr,\n",
    "    'Lasso': results_lasso,\n",
    "    'DTR': results_dtr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are trained, we can check how they perform using various metrics. Let's load the submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metrics in regression problems are:\n",
    "\n",
    "* the mean absolute error: $ MAE = |y_{true}-y_{pred}| $ \n",
    "* the root mean square error: $ RMSE = \\sqrt{y^2_{true}-y^2_{pred}}$\n",
    "\n",
    "where the $y_{true}$ is the vector of true values (or `test_labels` in our case) and the $y_{pred}$ are the predicted values for a given model. \n",
    "\n",
    "> **NB:** You can also get the percentage versions of the metrics by dividing the value by $y_{pred}$. Be careful though! It is dangerous to do so when the labels are close to zero as the error might skyrocket even for accurate predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that takes the results and the labels and outputs the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model:str, results:np.ndarray, labels:np.ndarray) -> None:\n",
    "    \n",
    "    # Calculate the metrics using the labels and the results\n",
    "    mae = metrics.mean_absolute_error(labels, results)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(labels, results))\n",
    "    mape = np.mean(mae / labels)\n",
    "    \n",
    "    # Print out the results\n",
    "    print(f'Results for model {model}: \\n- MAE:\\t{mae:.02f}\\n- RMSE:\\t{rmse:.02f}\\n- MAPE:\\t{mape*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise:** Try to calculate the normalized MAE defined as the MAE value divided by the average output value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in results.items():\n",
    "    print_results(model=name, results=model, labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_lasso, 'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to plot the other models. *Can you create a plot that has all of them?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model performs the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You developed your first ML model. 20% error is not that great though, we might have to go back in the loops to improve our results...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going back the loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and benchmark comes the hard part, we have to evaluate our work. If our model's performance is satisfactory, we are done. But unfortunatly our model's performance is not that great.\n",
    "\n",
    "**Because of that, we have to take a few steps backwards...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of options to improve the performance of our ML system. One idea is to try out different models. Maybe there are some that could work better with our dataset. But as we've seen, the difference in performance is not that big model-to-model. \n",
    "\n",
    "To boost the performance, we have some options, such as:\n",
    "\n",
    "- explore and benchmark different models, check if it should be more robust or smaller, \n",
    "- explore the data and find new relations between features,\n",
    "- explore the problem deeper, try to relate it to real life and understand what are the natural correlations between phenomena to add new features.\n",
    "\n",
    "E.g., understanding what impacts the energy price in real life leads to creation of better predictive models.\n",
    "\n",
    "For now, let's experiment by adding some features to the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated and mixed features\n",
    "\n",
    "Sometimes simple mathematical operations on existing features can create new one that are stronger, let's check the correlation table again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.corr()['price actual'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fossil fuels have an impact on increasing the price, on the other hand the renewables reduce it. \n",
    "\n",
    "Let's create new columns with a sum of fossils and alternative fuels to check what is the total impact they have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with all the fossil fuels\n",
    "\n",
    "fossil_list = [\n",
    "    'generation fossil hard coal',\n",
    "    'generation fossil gas',\n",
    "    'generation fossil brown coal/lignite',\n",
    "    'generation fossil oil']\n",
    "\n",
    "# Create lists with all the alternative fuels\n",
    "\n",
    "alternatives_list = [\n",
    "    'generation other', \n",
    "    'generation other renewable',\n",
    "    'generation solar', \n",
    "    'generation hydro water reservoir', \n",
    "    'generation nuclear', \n",
    "    'generation hydro run-of-river and poundage', \n",
    "    'generation biomass',\n",
    "    'generation wind onshore',\n",
    "    'generation hydro pumped storage consumption']\n",
    "\n",
    "data_energy['generation fossil total'] = data_energy[fossil_list].sum(axis=1)\n",
    "data_energy['generation alternatives total'] = data_energy[alternatives_list].sum(axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's calculate the share they have in the total energy mix at a given hour (we simply divide the sum of load by the total load):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy['generation alternatives share'] = data_energy['generation alternatives total'] / data_energy['total load actual']\n",
    "data_energy['generation fossil share'] = data_energy['generation fossil total'] / data_energy['total load actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the impact of the new features we added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.corr()['price actual'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, the `'total'` and `'share'` columns have quite high correlations with the price. Let's move to time-based feautures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.drop(columns=fossil_list+alternatives_list, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time features\n",
    "\n",
    "Adding some time-based features might boost the model's performance if there are some linear temporal dependencies in our model. Let's quickly add some time-based featuers by using our fancy date time index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy['hour'] = data_energy.index.hour            # Adds hour column\n",
    "data_energy['day_of_week'] = data_energy.index.weekday  # Adds weekday column\n",
    "data_energy['month'] = data_energy.index.month          # Adds month column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that what we added is correct, we can quickly draw plots of the average price per hour, per week day and per month using grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 3), sharey=True)\n",
    "\n",
    "ax[0].plot(data_energy.groupby('hour')['price actual'].mean())\n",
    "ax[0].set_xlabel('Hour')\n",
    "ax[1].plot(data_energy.groupby('day_of_week')['price actual'].mean())\n",
    "ax[1].set_xlabel('Day of week')\n",
    "ax[2].plot(data_energy.groupby('month')['price actual'].mean())\n",
    "ax[2].set_xlabel('Month')\n",
    "\n",
    "ax[0].set_ylabel('Price (€/MW)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trends are very clear!\n",
    "\n",
    "But... they are not exactly linear, so the model might have problems findning the correct weights for them. This is why categorical features are often introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features\n",
    "\n",
    "As the name implies, the categorical features specify a certain **category** to which each input belongs.\n",
    "\n",
    "Let's consider for example the days of the week, we can clearly see that the energy price is quite constant throughout the workdays, but decreases in Saturadays and Sundays. \n",
    "\n",
    "To help the model use that information, we can create new column where a **category** is added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_workday(X: pd.Series):\n",
    "    weekdays_list = []\n",
    "\n",
    "    for day in X:\n",
    "        if day < 5:  # We start the count at 0\n",
    "            weekdays_list.append('weekday')\n",
    "        elif day == 6:\n",
    "            weekdays_list.append('sat')\n",
    "        else:\n",
    "            weekdays_list.append('sun')\n",
    "    \n",
    "    return weekdays_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the function and check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy['weekdays'] = is_workday(X = data_energy.day_of_week)\n",
    "data_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the hours, as the trend is again highly non linear:\n",
    "\n",
    "- It's higher in the business and rush hours\n",
    "- It's lower in the middle of the day (siesta time)\n",
    "- It's even lower in the night\n",
    "\n",
    "Let's create another category that uses that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rush_hour(X: pd.Series):\n",
    "    hours_list = []\n",
    "\n",
    "    for hour in X:\n",
    "        if ((hour > 8 and hour < 13) or (hour > 17 and hour < 21)): # Between 9-14 and 18-22\n",
    "            hours_list.append('rush_hour')\n",
    "        elif (hour >= 13 and hour <= 17):\n",
    "            hours_list.append('siesta_hour')\n",
    "        else:\n",
    "            hours_list.append('night_hour')\n",
    "\n",
    "    return hours_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy['rush_hours'] = is_rush_hour(data_energy.hour)\n",
    "data_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "However, we can't just feed the category labels to the model. It would not know what to do with text.\n",
    "\n",
    "For that reason, we have to perform **encoding**, and more specifically the One-Hot encoding.\n",
    "\n",
    "OneHot Encoding transforms the input column into a number columns equal to the number of categories. Each column corresponds to one category. For each row the correct category column is marked with 1 (hot) and the rest are left as 0 (cold).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/one_hot.png?raw=true\" height=\"300\">\n",
    "\n",
    "\n",
    "This method allows the model to give independent weights to different categories.\n",
    "\n",
    "Fortunately, there is a `sklearn` implementation of this method. Let's declare it, transform the categorical columns from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', categories='auto', sparse_output=False) # Load the engine\n",
    "encoder.set_output(transform='pandas')                                                                 # Set the output to a pandas df\n",
    "inputs_categorical = encoder.fit_transform(data_energy[['weekdays', 'rush_hours', 'month']])           # Transform the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we end up with a table like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the 'text' category columns from our df. We don't need them anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.drop(columns=['weekdays', 'rush_hours'], inplace = True)\n",
    "data_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary features\n",
    "\n",
    "*Extra topic*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can support our predictions by importing and processing additional data from other sources. Of course this significantly adds to the workload, but often it is necessary to combine various data sources to get good quality of the prediction.\n",
    "\n",
    "In the beginning of the lesson we imported a secondary dataset that contains weather data for 5 big cities in Spain, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the names of the cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather.city_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 cities: Madrit, Barcelona, Valencia, Seville, and Bilbao. For convenience, let's move each city into separate `Dataframes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_valencia = data_weather[data_weather['city_name'] == 'Valencia'].copy()\n",
    "data_weather_madrid = data_weather[data_weather['city_name'] == 'Madrid'].copy()\n",
    "data_weather_bilbao = data_weather[data_weather['city_name'] == 'Bilbao'].copy()\n",
    "data_weather_barcelona = data_weather[data_weather['city_name'] == ' Barcelona'].copy()\n",
    "data_weather_seville = data_weather[data_weather['city_name'] == 'Seville'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's put them into a single dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_data_weather = {\n",
    "    'Valencia': data_weather_valencia, \n",
    "    'Madrid': data_weather_madrid, \n",
    "    'Bilbao': data_weather_bilbao, \n",
    "    'Barcelona': data_weather_barcelona, \n",
    "    'Seville': data_weather_seville\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As they are time series datasets, we should remember to add the date time index. We can do it in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, data in dictionary_data_weather.items():\n",
    "    \n",
    "    # Add TS index and remove the column\n",
    "    data.index = pd.to_datetime(data['dt_iso'], utc=True)\n",
    "    data.drop(columns='dt_iso', inplace=True)\n",
    "\n",
    "    # Update the data\n",
    "    dictionary_data_weather[city] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying all 5 datasets at the same time might be dificult, let's focus only on the Madrid data for now.\n",
    "\n",
    "We can connect the madrid dataset with the energy prices from the previous dataset using the `.merge()` method. Merging operation is very useful when we want to make sure that the resulting dataset will have the desired shape. The datasets are connected through a key (a column) that is shared between both sources.\n",
    "\n",
    "We have to specify the shared column (in our case the TS index) and the merging method ('inner' - keeps shared rows, 'outer' - keeps all rows, 'left' - keeps all rows from the left table, 'right' - the oposite of left). \n",
    "\n",
    "<img src=\"https://datacomy.com/data_analysis/pandas/merge/types-of-joins.png\" height=\"200\">\n",
    "\n",
    "\n",
    "At the first glance merging might be quite complex. But don't worry, we won't use it much. If you want more information on methods of connecting two data sources, have a read [here](https://realpython.com/pandas-merge-join-and-concat/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging with the price actual series it appears at the end of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_madrid.merge(data_energy['price actual'], how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save it as a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_madrid_with_price = data_weather_madrid.merge(data_energy['price actual'], how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the price included in the dataset, we can check if it has any correlation with the weather parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_madrid_with_price.corr(numeric_only=True)['price actual'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation are not that strong, but in reality probably both the temperature and the wind will have some impact on the results. Let's extract them. As we have 5 cities we can either get them separately, or aggregated. \n",
    "\n",
    "Let's try aggregated first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, data in dictionary_data_weather.items():\n",
    "    \n",
    "    print(f\"There are {data.shape[0]} observations about city: {city}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, data in dictionary_data_weather.items():\n",
    "\n",
    "    clean_data = data.drop_duplicates(subset='dt_iso', keep='first')\n",
    "\n",
    "    dictionary_data_weather[city] = clean_data\n",
    "\n",
    "    print(f\"There are {clean_data.shape[0]} observations about city: {city}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the aggregation let's use the city's population as the weight for a weighted average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_madrid = 6_791_667\n",
    "pop_barcelona = 5_474_482\n",
    "pop_valencia = 2_522_383\n",
    "pop_seville = 1_519_639\n",
    "pop_bilbao = 1_037_847\n",
    "\n",
    "pop_total = pop_madrid + pop_barcelona + pop_valencia + pop_seville + pop_bilbao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the weight by dividing the city's population by the total population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_weights_temp = {\n",
    "    'Valencia': pop_valencia / pop_total, \n",
    "    'Madrid': pop_madrid / pop_total, \n",
    "    'Bilbao': pop_bilbao / pop_total, \n",
    "    'Barcelona': pop_barcelona / pop_total, \n",
    "    'Seville': pop_seville / pop_total\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can create the `'temp'` and `'wind_speed'` columns as a weighted average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_weather = pd.DataFrame()\n",
    "\n",
    "inputs_weather['temp'] = \\\n",
    "    dictionary_data_weather['Valencia']['temp'] * dictionary_weights_temp['Valencia'] + \\\n",
    "    dictionary_data_weather['Madrid']['temp'] * dictionary_weights_temp['Madrid'] + \\\n",
    "    dictionary_data_weather['Bilbao']['temp'] * dictionary_weights_temp['Bilbao'] + \\\n",
    "    dictionary_data_weather['Barcelona']['temp'] * dictionary_weights_temp['Barcelona'] + \\\n",
    "    dictionary_data_weather['Seville']['temp'] * dictionary_weights_temp['Seville']\n",
    "\n",
    "inputs_weather['wind_speed'] = \\\n",
    "    dictionary_data_weather['Valencia']['wind_speed'] * dictionary_weights_temp['Valencia'] + \\\n",
    "    dictionary_data_weather['Madrid']['wind_speed'] * dictionary_weights_temp['Madrid'] + \\\n",
    "    dictionary_data_weather['Bilbao']['wind_speed'] * dictionary_weights_temp['Bilbao'] + \\\n",
    "    dictionary_data_weather['Barcelona']['wind_speed'] * dictionary_weights_temp['Barcelona'] + \\\n",
    "    dictionary_data_weather['Seville']['wind_speed'] * dictionary_weights_temp['Seville']\n",
    "\n",
    "inputs_weather['wind_speed_squared'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save it as a separate dataframe and concatenate with the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy = pd.concat([data_energy, inputs_weather], axis=1)\n",
    "data_energy.dropna(inplace=True)\n",
    "data_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind speed has a pretty high negative correlation with the price. It's quite strightforward, the more it blows the stronger the projection of wind energy and the lower the price. Also, probably, strong winds in summer decrease the energy load from cooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy.corr()['price actual'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with new features in hand, we can redo the initial calculations. \n",
    "\n",
    "The steps should be all familiar.\n",
    "\n",
    "First, we separate the inputs from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = data_energy.columns.to_list()\n",
    "input_columns.remove('price actual')\n",
    "label_columns = ['price actual']\n",
    "\n",
    "inputs = data_energy[input_columns].copy()\n",
    "labels = data_energy[label_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we scale the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.set_output(transform='pandas')\n",
    "\n",
    "inputs_scaled = scaler.fit_transform(inputs)\n",
    "labels_scaled = labels.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and add the One-Hot categorical table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_scaled = inputs_scaled.merge(inputs_categorical, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And separate the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(inputs) \n",
    "cutoff = int(0.7*n)\n",
    "\n",
    "X_train = inputs_scaled[:cutoff]\n",
    "y_train = labels_scaled[:cutoff]\n",
    "\n",
    "X_test = inputs_scaled[cutoff:]\n",
    "y_test = labels_scaled[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the final shape of the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can retrain the model. \n",
    "\n",
    "**Excercise:** Pick any model you want, train it with `.fit()` and predict with `.predict()`. Print the results with `print_results()` and plot them with `plot_results()` functions that we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done! We did a lot of work for quite a small improvement. Sadly, that is often the reality of Data Science... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Theoretical background on used models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Min-Max scaling - rescales the values to a slected range (usualy 0-1). For 0-1 range the equation takes the form:\n",
    "\n",
    "$$\n",
    "    X_{scaled} = \\frac{(X - min(X))}{max(X) - min(X)}\n",
    "$$\n",
    "\n",
    "- Standard scaling - rescales the values by removing the mean and scaling to unit variance (in statistical terms we calculate the z-scores):\n",
    "\n",
    "$$\n",
    "    X_{scaled} = \\frac{X - \\mu_x}{\\sigma_x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably all know Linear Regression by now. It is a very common statistical model that is often used as an introductory model to Machine Learning.\n",
    "\n",
    "In Linear Regression the relationship of the output of the model (or the prediction) to the descriptive variables (or the inputs) is modeled as a linear response, which mathematically can be written as:\n",
    "\n",
    "$$\n",
    "    y_i = w_0 + w_1 x_{i1} + w_2 x_{i2} ... + w_p x_{ip} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where the $y_i$ is the $i-th$ output, $x_1$ to $x_p$ are the features, and $w_1$ to $w_p$ are corresponding learned weights. The $\\epsilon$ is a random noise, that has to be taken into account in modeling. This equation in matrix terms becomes simpler:\n",
    "\n",
    "$$\n",
    "    \\mathbf{y} = \\mathbf{X W} + \\mathbf{\\epsilon}\n",
    "$$\n",
    "\n",
    "Fitting such a model becomes a task of finding the parameter matrix $\\mathbf{W}$ so that the term $ \\mathbf{\\epsilon} = \\mathbf{y} - \\mathbf{X W}$ is minimal.\n",
    "\n",
    "The classic method of solving this task is through the Ordinary Least Squares method (OLS). In practise the method is minimizing the sum of squares of the differences between the observed dependent variable (or outputs) and the modeled outputs. The objective function for the minimization is given as:\n",
    "\n",
    "$$ \n",
    "    S(W) = min|| \\mathbf{y} - \\mathbf{X W} || ^ 2\n",
    "$$\n",
    "\n",
    "Often it's easier to visualise the procedure as fitting a line, so that the sum of distances between the line and the observed points is the smallest possible:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1280/1*nhGPRU12caIw7NK5Rr3p-w.gif\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image by [Logan Yang](https://medium.com/swlh/from-animation-to-intuition-linear-regression-and-logistic-regression-f641a31e1caf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is a modification of Linear regression that introduces $L_1$ regularization. It is often preferred when we have more features as it tends to produce solutions with less non-zero features.\n",
    "\n",
    "The only difference from the standard LR is the new parameter in the objective funtion:\n",
    "\n",
    "$$ \n",
    "    S(W) = min \\frac{1}{2 n_{samples}} || \\mathbf{y} - \\mathbf{X W} || ^ 2 + \\alpha |w|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is a very simple algorithm that is principally used in classification. However, there exists also a quite conveniant regression implementation that is widely used due to its simplicity. KNN defines the output label by the distance to the nearest neighbors of the point in the mathematical space.\n",
    "\n",
    "In other words, it calculates the distance between our point and all the other points in the system. Then it orders the points descendingly according to the calculated distance. The output label is estimated on the basis of kept neighbors. Usualy, the distance is calculated as the Euclidean norm:\n",
    "\n",
    "$$\n",
    "    ||x|| = \\sqrt{x_1^2 + ... + x_n^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://images.datacamp.com/image/upload/v1686762721/image2_a2876c62d1.png\" height=\"300\">\n",
    "\n",
    "Image by [DataCamp](https://www.datacamp.com/tutorial/k-nearest-neighbors-knn-classification-with-r-tutorial)\n",
    "\n",
    "In regression the output is the mean of the neighbors' outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is again mainly a classification algorithm. In training the model grows a decision tree, with decision nodes and output leaves. When making prediction we always start at the *root node* and at each division, the model asks itself something about the dataset, e.g. is our fossil generation higher than 20GW?, is our hydro generation lower than 5GW? and based on a sequence of responses arrives at an output *leaf node*\n",
    "\n",
    "In classification the quality of the divisions is decided using a metric called *gini impurity*. A nore is pure, the more uniform it is in terms of class division. \n",
    "\n",
    "In case of regression the division is decided based on the *mean squared error*.\n",
    "\n",
    "Decision tree models are very prone to overfitting if the leaves are not *pruned*. We have to limit the depth of the tree or the number of nodes, or it will eventually grow to find a separate *leaf node* for each input sample (and that is severe overfitting)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ElW-ERvIfiV6RSbs74RO_A.png\" height=\"300\">\n",
    "\n",
    "Image by Alan Jeffares via [Medium](https://towardsdatascience.com/decision-trees-60707f06e836)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
