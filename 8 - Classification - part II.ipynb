{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced classification\n",
    "\n",
    "<img src=\"https://www.mrtfuelcell.polimi.it/images/logo_poli.jpg\" height=\"200\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg\" height=\"150\">\n",
    "\n",
    "A2A ML Course - day 8 - 18/11/2024\n",
    "\n",
    "Maciej Sakwa, Micheal Wood, Emanuele Ogliari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Introduction to image processing\n",
    "2. Training a CNN for image classification\n",
    "3. Object detection\n",
    "4. Image segmentation\n",
    "\n",
    "## Learning obejctives\n",
    "\n",
    "* Understand the mathematical concepts behind image processing\n",
    "* Learn to construct simple Convolutional Neural Network\n",
    "* Understand the computational burden of training large DL models, and the advantage of Transfer Learning\n",
    "* Provide an overview of the methodologies used in modern Computer Vision\n",
    "\n",
    "<img src=\"https://freesvg.org/img/evil-robot-glitch-remix.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please run the cells below with imports and function declarations** \n",
    "\n",
    "Feel free to skip the details of the content.\n",
    "\n",
    "---\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, accuracy_score, recall_score\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Input, Flatten, Resizing\n",
    "from tensorflow.keras import Sequential as create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RGB_layers(image, show_colors=False):\n",
    "    colors = ['Reds', 'Greens', 'Blues']\n",
    "    fig, ax = plt.subplots(ncols=3)\n",
    "    \n",
    "    for i in [0, 1, 2]:\n",
    "        if show_colors: \n",
    "            ax[i].imshow(image[:, :, i], cmap=colors[i], vmin=0, vmax=255)\n",
    "        else:\n",
    "            ax[i].imshow(image[:, :, i], cmap='Greys', vmin=0, vmax=255)\n",
    "        ax[i].xaxis.set_visible(False)\n",
    "        ax[i].yaxis.set_visible(False)\n",
    "        ax[i].set_title(colors[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_convolution(image):\n",
    "    n_filters = image.shape[-1]\n",
    "    fig, ax = plt.subplots(ncols=4, nrows=image.shape[-1]//4)\n",
    "    for i in range(n_filters):\n",
    "        ax[i//4, i%4].imshow(np.reshape(image[:, :, :, i], image.shape[1:-1]))\n",
    "        ax[i//4, i%4].xaxis.set_visible(False)\n",
    "        ax[i//4, i%4].yaxis.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_from_vid(source:str, out:str, n_frames=1, increment=1):\n",
    "    \n",
    "    # Check out path and create if does not exist\n",
    "    if not os.path.exists(out):\n",
    "        os.makedirs(out)\n",
    "\n",
    "    # Get the frame range and set current to zero\n",
    "    frames_range = range(0, n_frames*increment, increment)\n",
    "    current_frame = 0\n",
    "\n",
    "    # Open the video\n",
    "    vidcap = cv2.VideoCapture(source)\n",
    "\n",
    "    # Loop through the video saving the frames we want\n",
    "    while(True):\n",
    "        _ ,frame = vidcap.read()\n",
    "\n",
    "        if current_frame in frames_range:\n",
    "            name = f'test_img_{current_frame}.jpg'\n",
    "            print (f'Creating... {name}')\n",
    "            cv2.imwrite(os.path.join(out, name), frame)\n",
    "\n",
    "        current_frame += 1\n",
    "\n",
    "        if current_frame > n_frames:\n",
    "            break\n",
    "\n",
    "    # Release all space and windows once done\n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you run all the cells?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to image processing\n",
    "\n",
    "This lesson is a camouflaged tutorial on basics of Computer Vision (or CV in short). In fact, most of the tasks in CV center on classification of the contents of the image - we want to teach the machine to recognise some things that are generally easily recognisable for a human.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/01/image-classification-model.jpg\" width=\"600\">\n",
    "\n",
    "So far we dealt with simple datasets:\n",
    "\n",
    "* **Points**\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/linreg.png?raw=True\" width=\"400\">\n",
    "\n",
    "* **Lines** (sequences)\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/price_plot.png?raw=True\" width=\"400\">\n",
    "\n",
    "Where a single instance is usually a few numbers long, e.g. *(x, y, z)* coordinates or *(I, V, T, SOC*) features of time series. Now we will move on to images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a satellite image dataset, available [here](https://github.com/phelber/eurosat). Let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nielsr/eurosat-demo\", split='train').with_format('tf')\n",
    "ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "ds_class_names = {\n",
    "    0: 'AnnualCrop',\n",
    "    1: 'Forest',\n",
    "    2: 'HerbaceusVegetation',\n",
    "    3: 'Hihgway',\n",
    "    4: 'Industrial',\n",
    "    5: 'Pasture',\n",
    "    6: 'PermanentCrop',\n",
    "    7: 'Residential',\n",
    "    8: 'River',\n",
    "    9: 'SeaLake'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It comes in a conveniant format, already preprocessed and splitted into train and test.\n",
    "Each image has a label.\n",
    "\n",
    "Let's print the first example using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGE = 0\n",
    "\n",
    "image = ds['train'][N_IMAGE]['image'].numpy()\n",
    "label = ds['train'][N_IMAGE]['label'].numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the corresponding label is imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_class_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically speaking this small image is a massive 3D matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** Throughout the lesson we will probably use a lot the term *tensor*. A *tensor* is a generalisation of a matrix concept that can be expanded to more dimentions, e.g. 3D, 4D, ect... So a matrix is a 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image # .shape .size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is a square of 64x64 pixels.\n",
    "\n",
    "It has three layers that correspond to **(R, G, B)** channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGE = 0\n",
    "\n",
    "image = ds['train'][N_IMAGE]['image'].numpy()\n",
    "\n",
    "plot_RGB_layers(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each image has 12288 data points!**\n",
    "\n",
    "Previously we were talking about some descriptive parameters (current, voltage, temperature ect.) the number of *features* usually was below 10-20.\n",
    "\n",
    "A numercial representation of a single image on comparison will have thousands if not millions of *features*. A full HD image in *1920x1080* has a staggering 2073600 (2 million) data points!\n",
    "\n",
    "We cannot analyse them in the same way as we did before. Now it involves two techniques:\n",
    "\n",
    "* **CONVOLUTION**\n",
    "* **POOLING**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution is a mathematical operation that takes two functions $f$ and $g$ and creates a new function $(f*g)$ defined as:\n",
    "\n",
    "$$\n",
    "    (f * g) = \\int^{\\infty}_{-\\infty} f(\\tau) g(t-\\tau) d\\tau\n",
    "$$\n",
    "\n",
    "It's much easier to understand what's happening if we look at a graphical representation:\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/conv_book.png?raw=True\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce some terminology:\n",
    "* **Filter** or **kernel** - the computational unit that we *slide* along the input\n",
    "* **Stride** - number of pixels \"jumped\" at each *slide*\n",
    "* **Padding** - sides added to the image to equalize the size of the output with the input\n",
    "* **Feature map** or **map** - the output\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/conv_2d_book.png?raw=True\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is a 2-dimentional data structure, so the **kernel** (or filter) is also 2-dimentional.\n",
    "\n",
    "Normally, the kernel has a square shape: 2x2, 3x3, or sometimes 5x5, and even 7x7.\n",
    "\n",
    "The bigger the kernel the more general are the features extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a direct relation to ANNs:\n",
    "\n",
    "||**ANN**|**CNN**|\n",
    "|---|---|---|\n",
    "|**UNITS**|Neurons|Filters|\n",
    "|**LAYERS**|Hidden layer|Convolutional layer|\n",
    "\n",
    "We will be stacking layers of filters on top of each other. A single layer in a CNN will be composed of a number of filters (from single up to hundreds)\n",
    "\n",
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGE = 10\n",
    "\n",
    "image = ds['train'][N_IMAGE]['image'].numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolve function takes an image and applies the convolution using the number of filters that you select (here by default set to 16):\n",
    "\n",
    ">**NB:** it uses the `tensorflow` library to perform the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(image, n_filters=16):\n",
    "\n",
    "    if len(image.shape) != 4:\n",
    "        image = image.reshape([1, *image.shape])\n",
    "\n",
    "    convolution = tf.keras.Sequential([\n",
    "        Input(shape=image.shape[1:]),\n",
    "        Conv2D(filters=n_filters, kernel_size=2, padding='valid')\n",
    "    ])\n",
    "\n",
    "    return convolution(image)\n",
    "\n",
    "out_img = convolve(image, n_filters=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the maps using a previously defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convolution(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can stack the convolution operation many times.\n",
    "\n",
    "And this is exacly how complex features are extracted from the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_out_img = convolve(convolve(convolve(convolve(convolve(convolve(image))))))\n",
    "plot_convolution(out_out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting cool featuere maps now, because the weights on the filters are random.\n",
    "\n",
    "In fact, the training procedure of a CNN is about adjusting these weights to get maps that focus on specific parts of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "During training and inference, each filter has to pass over the input, moving one space at a time.\n",
    "\n",
    "Let's take the previously defined layer: 64x64x3 input, and 8 2x2 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 64 * 3 * 8 * 2 * 2 # Operations per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It quickly becomes a lot. Our PC memory will not be able to handle number of stacked layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling** is the necessary reduction operation, that downscales the image. Similarly a filter passes over the image, but now it outputs an aggregated result for each swept input.\n",
    "\n",
    "There are two types of pooling commonly used:\n",
    "\n",
    "* **Max** pooling\n",
    "* **Average** pooling\n",
    "\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/max-pooling-a.png?711b14799d07f9306864695e2713ae07\" height=\"300\">\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/average-pooling-a.png?58f9ab6d61248c3ec8d526ef65763d2f\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max pooling is more commonly used as it is better in preserving the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CNN for image classification\n",
    "\n",
    "### Putting the blocks together\n",
    "\n",
    "Now that we know the two most common building blocks of CNNs we can use them to construct the model.\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/fig-1-full.png?raw=True\" width=\"700\">\n",
    "\n",
    "The *rule of thumb* is that we:\n",
    "\n",
    "* put a few convolution layers with the same number of filters\n",
    "* put the max pooling layer to downsample\n",
    "* repeat a few times\n",
    "* flatten the output\n",
    "* attach an ANN at the end to predic the output\n",
    "\n",
    "Let's build a model using these rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = create_model([\n",
    "    Input(shape=(64, 64, 3)),\n",
    "\n",
    "    Conv2D(filters=16, kernel_size=(2, 2), padding='same'),\n",
    "    Conv2D(filters=16, kernel_size=(2, 2), padding='same'),\n",
    "    MaxPool2D(2),\n",
    "\n",
    "    Conv2D(filters=32, kernel_size=(2, 2), padding='same'),\n",
    "    Conv2D(filters=32, kernel_size=(2, 2), padding='same'),\n",
    "    MaxPool2D(2),\n",
    "\n",
    "    Conv2D(filters=64, kernel_size=(2, 2), padding='same'),\n",
    "    Conv2D(filters=64, kernel_size=(2, 2), padding='same'),\n",
    "    MaxPool2D(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(64),\n",
    "    Dense(len(ds_class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "test_model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's like putting LEGO blocks together :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model. As per usual, we split the data into train and test sets, and fit the model using the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds['train'].to_tf_dataset(columns=[\"image\"], label_cols=[\"label\"], batch_size=32)\n",
    "ds_test = ds['test'].to_tf_dataset(columns=[\"image\"], label_cols=[\"label\"], batch_size=1)\n",
    "\n",
    "train_history = test_model.fit(ds_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might take a couple of minutes. Let's run an example prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(ds_test.as_numpy_iterator()) # Get a single example from the test dataset\n",
    "\n",
    "example_image = example[0]  # Get image\n",
    "example_label = example[1]  # Get label\n",
    "\n",
    "plt.imshow(example_image[0])   \n",
    "print(ds_class_names[example_label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_probas = test_model.predict(example_image)\n",
    "label_pred   = ds_class_names[np.argmax(label_probas)]\n",
    "\n",
    "print(label_probas)\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the entire test dataset now and see the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true   = ds['test']['label']\n",
    "\n",
    "label_probas = test_model.predict(ds_test)\n",
    "label_pred   = np.argmax(label_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(label_true, label_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=ds_class_names.values())\n",
    "disp.plot(xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the numerical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy    = accuracy_score(label_true, label_pred)\n",
    "precision   = precision_score(label_true, label_pred, average=None)\n",
    "recall      = recall_score(label_true, label_pred, average=None)\n",
    "\n",
    "print(f'Global accuracy: {accuracy*100:.02f} %')\n",
    "print('\\t Prec\\t Recall')\n",
    "for i in range(len(ds_class_names)):\n",
    "    print(f'Cl {i}:\\t{precision[i]*100:.01f}%\\t {recall[i]*100:.01f}%\\t-> {ds_class_names[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods of putting the blocks together. How to be sure it's optimal?\n",
    "\n",
    "Mathematicians and computer scientists put out competitions to figure out who is the best at stacking blocks. \n",
    "\n",
    "<img src=https://m.media-amazon.com/images/I/71LMmS-xmdL._AC_UF894,1000_QL80_.jpg width=400>\n",
    "\n",
    "They are usually kind enough to share their findings online. Which makes it a bit useless to stack the blocks on our own in particular if we are doing a task that is fairly standard (such as image classification).\n",
    "\n",
    "These models are trained on very simple datasets.\n",
    "\n",
    "For example it learns to predict cats vs dogs vs horses. And what if we want to classify types of defects on PV modules? Can we use the same model?\n",
    "\n",
    "The answer is: *kind of*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a lot of training process is spent on the deep first layers of the CNN, that extract the most basic features. \n",
    "It *also* turns out that these layers will have the same weights no matter on the task we want to do with the model (dogs or PV modules).\n",
    "\n",
    "*We can use that* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact we can reuse the deeper and middle layers of any model that performs a similar task. \n",
    "\n",
    "Here, we load a MobileNet model pretrained on 'imagenet' dataset and remove the top hidden layers with include_top=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_layers = tf.keras.applications.MobileNetV3Small(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the the loaded layers to mold in into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model([\n",
    "    Input(shape=(64, 64, 3)),\n",
    "\n",
    "    Resizing(224, 224),\n",
    "\n",
    "    mobilenet_layers,\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(64),\n",
    "    Dense(len(ds_class_names), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *re*-fitting procedure is often called **fine-tuning**. It's enough if we run it just for a few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true   = ds['test']['label']\n",
    "\n",
    "label_probas = model.predict(ds_test)\n",
    "label_pred   = np.argmax(label_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And show them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(label_true, label_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=ds_class_names.values())\n",
    "disp.plot(xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy    = accuracy_score(label_true, label_pred)\n",
    "precision   = precision_score(label_true, label_pred, average=None)\n",
    "recall      = recall_score(label_true, label_pred, average=None)\n",
    "\n",
    "print(f'Global accuracy: {accuracy*100:.02f} %')\n",
    "print('\\t Prec\\t Recall')\n",
    "for i in range(len(ds_class_names)):\n",
    "    print(f'Cl {i}:\\t{precision[i]*100:.01f}%\\t {recall[i]*100:.01f}%\\t-> {ds_class_names[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure is often reffered to as **TRANSFER LEARNING** as we are transfering the knowledge from one model to a new one. It is commonly used if the task that we want to perform is similar to the one we already did.\n",
    "\n",
    "The most important models that intoduce new and cool ways of stacking blocks (and win the competitioms) are often named. For example:\n",
    "\n",
    "* VGG-16 nets - introduced the Conv-Pool stacking that we used before\n",
    "* Residual nets [*(Resnets)*](https://en.wikipedia.org/wiki/Residual_neural_network) - introduced residual blocks with skip connections.\n",
    "* Inception nets or [*GoogleNet*](https://en.wikipedia.org/wiki/Inception_(deep_learning_architecture)) - introduced inception block for wide and deep feature extraction\n",
    "* Squeeze-and-excitation [*(SE)nets*](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7) - expands on the ResNet by scaling the feature maps with learned weight parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use example in engineering\n",
    "\n",
    "Convolutional Neural Networks are extensively used in research and industry, for image based predictions. \n",
    "\n",
    "An example that we were working on (and now Nam is working), is the CNN based prediction of Global Horizontal Irradiance. In that scenario we used a modified VGG-16 type network for *regression* based on a sequence of images.\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/CNN_pred_example.png?raw=True\" width=\"800\">\n",
    "\n",
    "Apart from images, CNNs are also extensively used for sequence analysis - e.g. time series forecast or signal classification (such as speech recognition). We can do it because it is very easy to represent a window of a time series as a 2D matrix that can be *scanned* convolutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection\n",
    "\n",
    "Another important task in image processing is that of **Object Detection**. \n",
    "\n",
    "In comparison to classification, now the task is a bit different as our objective is not only about classifying the content of the image, but also about detection of the location of that object:\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/object-detection-clas-en.jpeg?raw=True\" width=\"250\">\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/object-detection-det-en.jpeg?raw=True\" width=\"250\">\n",
    "\n",
    "That means that the output of our model is not only the single label for the entire image but a set of **bounding boxes** with accompanying labels.\n",
    "\n",
    "The task changes from stright classification to *half-regression* (estimation of the location of the box) and *half-classification* (classification of the content of a box).\n",
    "\n",
    "The staple model to perform this task is called **YOLO** - **Y**ou **O**nly **L**ook **O**nce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webcam test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"yolov5su.pt\")\n",
    "\n",
    "# Model info\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By typing model.names we can see what classes it is learned to detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's 80 classes that come from another standard dataset called [*COCO*](https://cocodataset.org/#home) (Common Objects in COntext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains code to take pictures with your webcam. *Please run it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from IPython.display import display, Javascript\n",
    "    from google.colab.output import eval_js\n",
    "    from base64 import b64decode\n",
    "\n",
    "    def take_photo(filename='/content/photo.jpg', quality=0.8):\n",
    "        js = Javascript('''\n",
    "            async function takePhoto(quality) {\n",
    "            const div = document.createElement('div');\n",
    "            const capture = document.createElement('button');\n",
    "            capture.textContent = 'Capture';\n",
    "            div.appendChild(capture);\n",
    "\n",
    "            const video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "            document.body.appendChild(div);\n",
    "            div.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            // Resize the output to fit the video element.\n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "            // Wait for Capture to be clicked.\n",
    "            await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "            const canvas = document.createElement('canvas');\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            stream.getVideoTracks()[0].stop();\n",
    "            div.remove();\n",
    "            return canvas.toDataURL('image/jpeg', quality);\n",
    "            }\n",
    "            ''')\n",
    "        display(js)\n",
    "        data = eval_js('takePhoto({})'.format(quality))\n",
    "        binary = b64decode(data.split(',')[1])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(binary)\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a picture with our webcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from IPython.display import Image\n",
    "    # Take an image and display an error if something goes wrong\n",
    "    try:\n",
    "        filename = take_photo()\n",
    "        print('Saved to {}'.format(filename))\n",
    "        display(Image(filename))\n",
    "\n",
    "    except Exception as err:\n",
    "        str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try to see what the model detects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):   \n",
    "    input_img = \"/content/photo.jpg\"\n",
    "    results = model.predict(input_img, save=True)\n",
    "\n",
    "    results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyse in more detail the model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    classes = results[0].boxes.cls\n",
    "    conf    = results[0].boxes.conf\n",
    "    xyxy    = results[0].boxes.xyxy\n",
    "\n",
    "    pd.DataFrame({\n",
    "        'left': xyxy[:, 0], \n",
    "        'right': xyxy[:, 2], \n",
    "        'bottom': xyxy[:, 1], \n",
    "        'top': xyxy[:, 3],\n",
    "        'conf': conf,\n",
    "        'class': [model.names[int(cls.item())] for cls in classes]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a technical point of view the **YOLO** model is dividing the image in small patches and looking for anchor points of the objects in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the model is so fast we can use it in real time *(almost)*.\n",
    "\n",
    "Let's see how it handles videos. This cell should download a video from our github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://raw.githubusercontent.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/refs/heads/main/data/test_vid_small.mp4\"\n",
    "! wget --no-cache --backups=1 {url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the video to colab, give it a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import os\n",
    "\n",
    "# Show video\n",
    "mp4 = open(\"test_vid_small.mp4\",'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's predict using the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vid = model.predict(source=\"test_vid_small.mp4\", save=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use example in engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With proper *fine-tuning* and *re-training* on usable datasets, we can use YOLO to detect other objcets that we need. An example that we were working on includes fine-tuning the YOLO model to detect defects in PV modules for facilitated diagnosis of power drops: \n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/YOLO_pred_example.png?raw=True\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "The last important task in image processing is that of **Segmentation**. \n",
    "\n",
    "It is significantly more advanced compared to image classification, because here we try to classify **every pixel of the image**.\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/object-detection-clas-en.jpeg?raw=True\" width=\"250\">\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/object-detection-det-en.jpeg?raw=True\" width=\"250\">\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/r-cnn-en.jpg?raw=True\" width=\"250\">\n",
    "\n",
    "Just like **YOLO** is the staple model for Object Detection, **SAM** (Segment Anything Model) is the go-to solution for Segmentation. It's a robust vision models developed by some smart guys at META. In comparison to previous models we used and trained, **SAM** is an example of a *Vision Transformer* or *ViT* - a prompt-driven heavyweight model that is based on encoding and decoding of the input. *ViT* models are significantly too heavy to be trained \"in-house\". In fact, in the original research paper, the authors report that the training of only the encoder part took them a few *days* using a staggering number of **256 GPUs**! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract a couple of frames from our video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frames_from_vid(source=\"test_vid_small.mp4\", out=\"frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chech out the image in our content browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to **YOLO** the **SAM** model is hosted on ultralytics. We can easily import it and run predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "# Load a model\n",
    "model = SAM(\"mobile_sam.pt\")\n",
    "\n",
    "# View model info\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's massive compared to **YOLO**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run it with the .predict() command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source='frames/test_img_0.jpg', save=True, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This however takes a very long time to run in colab (slightly over 7 minutes for full picture).\n",
    "\n",
    "Moreover, the output is not very clear, you can see the example below:\n",
    "\n",
    "<img src=\"https://github.com/woodjmichael/Basi-Fondamentali-del-Machine-Learning/blob/main/images/SAM_result.jpeg?raw=True\" width=\"900\">\n",
    "\n",
    "It's a mess, isn't it? \n",
    "\n",
    "The model however was designed for \"prompted\" segmentation, i.e. we have to specify which point or area of the image we want to extract, e.g. we can focus on one item location, or on detection of objects on the path of our car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source='frames/test_img_0.jpg', save=True, device='cpu', points=[[100, 100]], labels=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not working perfect, but that's what fine-tuning's for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thank you for your attention!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
